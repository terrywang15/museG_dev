<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called background.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from 01-background.Rmd.
-->

<!--
The {#background} text after the chapter declaration will allow us to link throughout the document back to the beginning of of the background section.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Background {#background .unnumbered}

**Introduction** 

We propose the application of Deep Learning models to help individuals generate interesting music ideas, with the goal of aiding musicians to use these computer-generated musical ideas to enhance their music writing process. Our model focuses on generating melody instead of the sound. We use midi files consisting of music notes, as training data. In addition, we explore a unique model structure where the time dependency inherent in music is not explicitly given by the model structure but is inferred by the model through the deep neural net structure. Despite this, we have observed that the model was able to learn time-dependent musical structure, such as the pre-dominant to dominant to tonic chord progression. 

We note that the model exhibits certain unconventional behavior. This can be attributed to the model using a different perspective to create music compared to humans. This behavior may be useful in inspiring musicians to create interesting from a different perspective.

**Purpose** 

Our purpose is to build an effective platform to help artists in their creative process and assist them in music composition. Note that artists do not have to be professionals. Any individual interested in creating music should find the platform useful and empowering. The first concrete step we need to take to achieve that goal is to build a music generation model that is able to generate music that is similar to what a human would compose in the style of music the model is trained on.

To provide some historical context, consider in 1950s that experimental music composers wrote music using randomized statistical modelling. In 1990s, David Bowie built the Verbasizer, which implemented a random re-ordering of groups of words and sentences to produce potentially significant lyrical combinations. A 17 seconds long melody named "The Silver Scale" was the first ever music generated by computer, which was in the year 1957 [@silver_scale]. Some of the very early algorithms used stochastic Markov Chain models for generation and rule filtering [@temperley2007].

Since then, computer music has been gaining a lot of public attention. In present time, there is an entire industry built around AI generated music including Flow Machines [@flow_machines], IBM Watson Beat [@watson] and Googleâ€™s NSynth [@nsynth]. 

\newpage
