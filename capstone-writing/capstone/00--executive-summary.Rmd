
We propose the application of Deep Learning to generate interesting music ideas to aid individuals in their music creation process. The generated music sample results show some promise and bring us a step closer to reaching our goal of useful musical output. We include a few ideas on the future improvement of the model training process.

Our primary model is a GAN with two components: a generator and a discriminator (also called critic). The generator takes a random input and outputs a midi sample (a file type for music). The discriminator takes a midi sample as the input and outputs a classification of real or fake. For real samples, we used a midi data repository containing 130,000 midi files across many different genres of music. For this project, we specifically used Baroque music. We structured our data with two main components in mind: the music note and the duration of the note. The `note` was one-hot encoded with 128 possible notes. The duration of the note was broken out by `start_time` and `end_time`, which are positive float vectors. This structure seems to perform the best out of the many we tried.

Some other models we tried used time series prediction of music notes, including an RNN model that uses a grouping of three notes as input to predict the next three notes.

In order to perform the necessary pre-processing of the data, our work includes a modified version of the python package pretty_midi. We adapted pretty_midi's main engine to manipulate midi files with varying tempo and key changes, adapt midi files into data structures usable by python, and reverse-adapt our output back into a midi that can be used by musical software. Lastly, we provided an assortment of utility functions and model selections, which we made into a python package together with the pretty_midi.

\bigskip
\bigskip
\bigskip


