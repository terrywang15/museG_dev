# Conclusion {.unnumbered}

The deep neural network structure implemented in our generator was able to generate human-like musical output that exhibits understanding of music theory, Baroque style, pattern recognition, and a preference to generate passages that is self-referencing. It shows great promise to become an additional tool for musicians to generate new musical ideas of certain styles. 

The model in the current state is plagued by the inability to maintain stable learning and the problem of mode collapse, which are related problems. We propose that in our next steps, we keep iterating on the model structure to improve on these issues.




# Next Steps {.unnumbered}

We see the following directions that we can take to further improve our model:

1. Make training data more consistent in terms of style and music instrument. We believe that the more consistent the input, the better and faster the generator can learn. However, this is a quite time-consuming task to sift through each and every midi file to find which composers, passages and instruments to include.
2. Upgrade model structure. While our relatively simplistic model structure is able to achieve quite a bit, we believe that adding additional complexity will marginally increase the performance of the model. We can also investigate the impact of increasing layers to our baseline generator or discriminator.
3. Implement new training script that is able to monitor losses. This would involve the development of a loss stability metric and stops training when loss instability is detected.
4. Implement flexible generator/discriminator in conjuction with loss monitoring. This idea is discussed in the Findings section and will enable each layer of the model to learn as much as possible before loss starts to increase, thus avoiding the problem of loss increase.
