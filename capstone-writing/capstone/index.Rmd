---
author: 'Terry Wang, Rima Mittal, Joshua Goldberg'
date: 'March, 2020'
institution: 'University of Chicago'
division: 'Graham School'
advisor: 'Yuri Balasanov'
altadvisor: ''
department: 'Continuing Liberal and Professional Studies'
degree: 'Master of Science in Analytics'
title: 'musG_dev, the Deep Music Generator'
knit: "bookdown::render_book"
header-includes:
    - \usepackage{setspace}\doublespacing
site: bookdown::bookdown_site
output: 
  phoenixdown::capstone_pdf: default
#  phoenixdown::capstone_gitbook: default
#  phoenixdown::capstone_word: default
#  phoenixdown::capstone_epub: default
#
# If you are creating a PDF you'll need to write your preliminary content as well as define some other parameters below.
abstract: | 
  `r if(knitr:::is_latex_output()) paste(readLines("00--abstract.Rmd"), collapse = '\n  ')` 
executive: |  
  `r if(knitr:::is_latex_output()) paste(readLines("00--executive-summary.Rmd"), collapse = '\n  ')` 
#
# Longer preliminary content, like the Abstract and Executive Summary above, is best organized in seperate files.
# The inline r function is used above to paste the contents of those files, instead of requiring you one to type 
# lengthy text directly into the yaml header. For shorter messages, typing directly into the YAML is easier. See below.
# VERY IMPORTANT: A tab indent is needed on the line following the |.
#
# preface: |
 # A preface is OPTIONAL. Use a preface if you want to explain your interest in the report topic and include anything about your experience that readers should keep in mind. If you would rather not include a preface, comment it out or delete it from the YAML header of the index.Rmd file.

acknowledgements: |
  We would like to thank Prof. Yuri Balasanov for his guidance and kind support during our investigations.
#dedication: |
#  You can have a dedication here if you wish.
#
# Download your specific bibliography database file, place it in the "bib" folder, and refer to it in the line below
bibliography: bib/thesis.bib
#
# To change your Citation Style Language file, you can do so below. Though the default is apa style.
csl: csl/apa.csl
lot: true
lof: true
#
# Add a "#" at the beginning of the following line if you'd like remove space between parapgraphs.
space_between_paragraphs: true
#
# Dimensions below correspond to University of Chicago Masters of Science in Analytics requirements.
geometry: "left=3.8cm, right=2.5cm, top=2.5cm, bottom=2.5cm"
#
  #header-includes:
#- \usepackage{tikz}
---



<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!

If you'd prefer to not include a Preface for example, simply delete lines 32-33 above or add a "#"" before them to comment out.  If you have other LaTeX packages you would like to include, delete the # before header-includes and list the packages after hyphens on new lines.

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this:
-->

<!--

If you receive a duplicate label error after knitting, delete the extra .Rmd file and then knit again.
-->

<!-- You'll need to include the order that you'd like Rmd files to appear in the _bookdown.yml file for PDF files and also delete the # before rmd_files: there. Do not include 00(two-hyphens)prelim.Rmd,  00(two-hyphens)abstract.Rmd and 00(two-hyphens)executive summary.Rmdsince they are handled in the YAML above differently for the PDF version.
-->

<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers on chapters, which is the standard for each section.
-->

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
```

# Introduction {.unnumbered}

## Problem Statement {.unnumbered}

A 17 seconds long melody named "The Silver Scale" was the first ever music generated by computer, which was in the year 1957. Some of the very early algorithms used stochastic Markov Chain models for generation and rule filtering. Since then, computer music has been gaining a lot of public attention. The objective is to assist human musicians (composers, arrangers, producers etc) with the help of computer based music generation environments.

The concept is to let the machine learn the music styles automatically with as little human intervention as possible. The motivation behind using Deep Learning models to generate music is its generality and flexibility. As opposed to a rule based or a grammar based system, Deep Learning can generate samples which inherit the music styles and introduce appropriate randomness. 

## Research Purpose {.unnumbered}

The purpose of this project is to develop a deep learning based music generator (instrumental music) which has a good understanding of the language of music and is able to generate human-like outputs, ultimately helping musicians and non-musicians alike to develop and refine musical ideas, and facilitate them in the music composition process. The model follows the Generative Adversarial Network structure, in which a generator and a discriminator (also called critic) compete to become better at generating human-like samples and finding fake samples, respectively. In contrast to more popular approaches that use explicit and artificial model structures to model time dependency and pattern recognition, we use a deep neural network structure for the generator which we have proven to achieve similar level of musical output.

During training, weights and parameters of the model will hopefully converge to a point at which the music generated is satisfactory to the user, who can use the output of the model, or save the weights for future use.  In a sense, we are democratizing deep learning to non-technical users wishing to generate new music ideas in a novel way. Moreover, we expect the algorithmic process to able to explore more music combinations than any individual, and thus provide a more holistic and creative approach to music creation.

## Variables and Scope {.unnumbered}

We used a midi dataset ( [see reddit page](https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_internet/)) ( [download](https://mega.co.nz/#!Elg1TA7T!MXEZPzq9s9YObiUcMCoNQJmCbawZqzAkHzY4Ym6Gs_Q)) consisting of 130,000 midi files from a wide range of genres included in this collection. We substantially narrowed down our training data to around 30 songs representative of the Baroque era.

Python libraries like Music21 and PrettyMIDI are the toolkits we used to extract meaningful information from our MIDI files. 

Some initial EDA led us to reading the music files as music21.stream.Score objects, which is a stream subclass for handling multi-part music. We were able to extract all partStreams in a Score using parts.stream.  

The Stream object can then be used to identify:
1. Instruments
2. Key Signatures
3. Overlaps
4. Time Signatures
5. Measures

From the measure, we were then able to identify:
1. Notes
2. Chord

Our initial EDA consisted of generating the Pitch Histogram and performing other Composition Parameters Analysis. We analyzes Music time Signature, Expected Music Key, Music Key Confidence and other Music Key Alternatives based on correlation values. The EDA helped us to narrow down on Baroque music because of its structural similarity and style saliency, so that we can better assesss the quality of the generator's output.

The final training data consists of a randomly sampled of 100,000 samples of 20 notes taken from the 30 Baroque songs. All of the generated samples are also capable of generating 20-note samples in the same format as the input data.
