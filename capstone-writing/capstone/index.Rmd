---
author: 'Terry Wang, Rima Mittal, Joshua Goldberg'
date: 'March, 2020'
institution: 'University of Chicago'
division: 'Graham School'
advisor: 'Yuri Balasanov'
altadvisor: ''
department: 'Continuing Liberal and Professional Studies'
degree: 'Master of Science in Analytics'
title: 'musG_dev, the Deep Music Generator'
knit: "bookdown::render_book"
header-includes:
    - \usepackage{setspace}\doublespacing
site: bookdown::bookdown_site
output: 
  phoenixdown::capstone_pdf: default
#  phoenixdown::capstone_gitbook: default
#  phoenixdown::capstone_word: default
#  phoenixdown::capstone_epub: default
#
# If you are creating a PDF you'll need to write your preliminary content as well as define some other parameters below.
abstract: | 
  `r if(knitr:::is_latex_output()) paste(readLines("00--abstract.Rmd"), collapse = '\n  ')` 
executive: |  
  `r if(knitr:::is_latex_output()) paste(readLines("00--executive-summary.Rmd"), collapse = '\n  ')` 
#
# Longer preliminary content, like the Abstract and Executive Summary above, is best organized in seperate files.
# The inline r function is used above to paste the contents of those files, instead of requiring you one to type 
# lengthy text directly into the yaml header. For shorter messages, typing directly into the YAML is easier. See below.
# VERY IMPORTANT: A tab indent is needed on the line following the |.
#
# preface: |
 # A preface is OPTIONAL. Use a preface if you want to explain your interest in the report topic and include anything about your experience that readers should keep in mind. If you would rather not include a preface, comment it out or delete it from the YAML header of the index.Rmd file.

acknowledgements: |
  We would like to thank our mentor, Prof. Yuri Balasanov, for his guidance and kind support during our investigations. 
  
  Prof. Mildred Rey provided valuable feedback to improve the style and content of this paper. We truly appreciate her help.
  
  Profs. Lawrence Zbikowski and Sam Pluta provided additional feedback. Thank you.
#dedication: |
#  You can have a dedication here if you wish.
#
# Download your specific bibliography database file, place it in the "bib" folder, and refer to it in the line below
bibliography: bib/thesis.bib
#
# To change your Citation Style Language file, you can do so below. Though the default is apa style.
csl: csl/apa.csl
lot: true
lof: true
#
# Add a "#" at the beginning of the following line if you'd like remove space between parapgraphs.
space_between_paragraphs: true
#
# Dimensions below correspond to University of Chicago Masters of Science in Analytics requirements.
geometry: "left=3.8cm, right=2.5cm, top=2.5cm, bottom=2.5cm"
#
  #header-includes:
#- \usepackage{tikz}
---



<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of metadata used to produce the document.  Be careful with spacing in this header!

If you'd prefer to not include a Preface for example, simply delete lines 32-33 above or add a "#"" before them to comment out.  If you have other LaTeX packages you would like to include, delete the # before header-includes and list the packages after hyphens on new lines.

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this:
-->

<!--

If you receive a duplicate label error after knitting, delete the extra .Rmd file and then knit again.
-->

<!-- You'll need to include the order that you'd like Rmd files to appear in the _bookdown.yml file for PDF files and also delete the # before rmd_files: there. Do not include 00(two-hyphens)prelim.Rmd,  00(two-hyphens)abstract.Rmd and 00(two-hyphens)executive summary.Rmdsince they are handled in the YAML above differently for the PDF version.
-->

<!-- The {.unnumbered} option here means that the introduction will be "Chapter 0." You can also use {-} for no numbers on chapters, which is the standard for each section.
-->

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
```

# Introduction {.unnumbered}

<!--change order of these two paragraphs or merge the two-->
<!--Finding a place to start to be the most challenging. Then talk about pain points. Then talk about the solution.-->

Perfecting the creative process is a goal across all forms of art (ex. music, photography, painting, singing, poetry, writing). Finding a place to start can be the most challenging (writer's block). A musician can go down a melodic path that leads to a dead end. The process of discovery and satisfaction by the creator is iterative trial and error. To use a model as an analogy, this would be similar to an infinite grid search for hyperparameters â€” something that is not practical in the field of machine learning. Astonishingly, musicians today have been able to create music with their bare intellect and network of inspiration.

While there are successful musicians in today's creative environment, we think there is always room for improvement in the creative process. This deep learning approach aims to be the catalyst for that improvement by handling the difficult and arduous task of discovery and ideation for musicians and individuals interested in creating music by generating new melodic ideas in a short amount of time based on the musician's preferences, dramatically enhancing the creative capacity of the musician.

The purpose of the project is to develop a deep learning-based music generator (instrumental music) that has a good understanding of the language of music and can generate human-like outputs. Ultimately, the generator supports musicians and non-musicians alike in developing and refining musical ideas. We divided the model phase of the project into two parts: Generation and Validation.

By doing this iteratively, the weights and parameters of the model will hopefully converge to a point at which the music generated is satisfactory to the user so they can use the output of the model or save the weights for future use. We expect the algorithmic process to able to explore more music combinations than any individual and thus provide a more holistic and creative approach to music creation.

At the current stage, we are focused on taking the first step towards our stated goal, and have created a deep learning model that shows great promise of generating human-like musical output.
