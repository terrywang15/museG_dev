# Methodology {#methodology .unnumbered}

The purpose is to build an effective platform to help artists in their creative process and assist them in music composition. Note that artists do not have to be professionals. Any individual interested in creating music should find the platform useful and empowering. The first concrete step we need to take to achieve that goal is to build a music generation model that is able to generate music that is similar to what a human would compose in the style of music the model is trained on. In this section we will dive deep into the details of how we preprocess the training data and how we set up the model for music generation.

## Data {#methodology-data .unnumbered}

The training data used to train our model is a collection of midi files that was compiled by a Reddit user [see reddit page](https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_internet/) and put up for [download](https://mega.co.nz/#!Elg1TA7T!MXEZPzq9s9YObiUcMCoNQJmCbawZqzAkHzY4Ym6Gs_Q). There are about 130,000 midi files from a wide range of genres included in this collection. Our training, however, centers around a small subset of the files that are of 4/4 rhythm, in order to reduce the chance of complication from training with a wider range of rhythmic patterns.

In addition, in our pre-processing script, we took steps to "normalize" the samples, in terms of making the tempo constant as well as transposing all songs to the key of C major (or A minor if in minor key). The efforts to normalize keys is not always successful. In some midi files, tempo and key changes are included as part of the metadata, which we can then extract and make changes accordingly. But in others, the metadata is not included in which case we do not adjust the keys.

In order to understand how to convert midi files into a format that can be an input to a deep neural network, it is necessary to introduce the basic structure of midi files. Midi files (.mid) are digital records of musical "events" divided into various tracks, where each track contains a voice, an instrument, or a line of melody. For example, a midi file containing piano music might have two tracks representing the notes played by the two hands of the human piano player. Each time the human player plays a note, the midi file would record the following information: which key is pressed; the time the key is pressed; the time the key is released; how hard the key is pressed; what the pitch bend is. After collecting this information for each and every note in a song in a midi file, we would have most of the information needed to reproduce the song in its entirety down to some basic expressive elements. In the midi metadata, some other relevant information is stored, such as tempo, key, tempo changes, etc. Midi files are not music by themselves because they only contain the information needed to produce music: it is the digital equivalent of sheet music. Midi decoders and synthesizers are needed to translate midi files into music.

If we strip the midi file structure down to its most basic elements, it would look something like this:

```{r, out.width="0.8\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Midi File Structure"), echo=FALSE}
knitr::include_graphics("figure/midi_file_structure.png")
```

We can imagine the midi file to be a dictionary, where each track is indexed by its name, and in each track we have a tabular structure of data consisting of the following columns: pitch number, start time, end time, and pitch velocity (we will ignore pitch bend for now). For example, if we look at the first three notes played by the right hand in Beethoven's Moonlight Sonata, we can tabulate them as follows:

| Pitch | Start Time | End Time | Pitch Velocity |
|-------|------------|----------|----------------|
| 56    | 0.00       | 0.40     | 33             |
| 61    | 0.40       | 0.81     | 26             |
| 64    | 0.81       | 1.21     | 26             |
Table: Midi File Structure Example

Table 1 tells us the following: At time 0.00 seconds, pitch 56 is played with velocity 33, and the note stopped at time 0.40 seconds; then at time 0.40 seconds pitch 61 is played with velocity 26, and the note stopped at time 0.81 seconds; and so on. We can see clearly how a piece of music can be represented digitally via the midi file structure.

Finally, different components of the midi file have different data types. The `Pitch` and `Pitch Velocity` columns take only an integer between 0-127, which is a discrete structure that required one-hot encoding. The `Start Time` and `End Time` columns are positive floats to represent the number of seconds since the start of the song. Since the goal of the model is to generate novel melody ideas, we decided to focus only on `Pitch`, `Start Time`, and `End Time` from the midi file information as well as limiting the output of the generator to 20 notes at a time. We made the decision to only focus on these variables in order to focus on learning the melody and not expressiveness as well as to cut down on complexity. Again, `Pitch` is the piano key pressed, and `Start Time` and `End Time` are time marks at which the keys are pressed. These features had an impact on the data pre-processing.

For data fed into the model, we use a processing script to rewrite a pre-selected subset of these midi files into numpy arrays in accordance with the above-mentioned structure: each song was represented by a series of numpy arrays, each representing a track, and each array will have the tabular structure of 130 columns: a 128-length one-hot vector for `Pitch`, one column for `Start Time`, and another column for `End Time`. We then use another training script to randomly make 20-note samples from the training data and arrange them into a numpy array. We need 20-note samples in order to make sure that the real and fake samples have the same dimensions.[^1] Therefore, our training dataset has dimensions of (x, 130, 20) where x represents the number of real samples generated for training, the number 130 represents the three columns (`Pitch`, `Start Time`, `End Time`), and 20 represents the 20 notes sampled.

[^1]: For our discriminator model this is necessary, but there are other model structures available to overcome this problem. For an example of a more elaborate structure see [@foster2019].

```{r, out.width="0.8\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Training Data Structure"), echo=FALSE}
knitr::include_graphics("figure/training_data_structure.png")
```

The decision to make the pitch a one-hot vector was made because in the Western musical tradition pitches do not exist on a linear or ordinal scale as we typically think of them. Musical theory does have a pattern and order, but for the purposes of modeling, this does not exist. One-hot encoding pitches will allow us to treat each pitch as and independent object. In addition, our model will be able to output softmax for pitches, therefore leading to pitch probabilities that can be further investigated.

\newpage

## Modeling Framework {#methodology-modeling .unnumbered}

Our model follows the Generative Adversarial Networks (GAN) architecture. GANs are a special case of deep learning models where the goal is to turn random noise into some kind of output (images or, in our case, music) which captures the essence of the training data. For example, they have been successfully deployed to generate realistic visual images.

```{r, out.width="0.8\\linewidth", include=TRUE, fig.align="center", fig.cap=c("GAN Structure"), echo=FALSE}
knitr::include_graphics("figure/GAN_structure.png")
```

The basic structure of the model has three parts: the generator, the discriminator, and the adversarial model, which combines the generator and the discriminator. The basic idea of the model is to train the discriminator with both real samples (from training data) and fake samples (generated by the generator), and then train the generator with the error from the discriminator. The goal is not to minimize the error of the generator or discriminator, but rather to achieve a balance between the generator and the discriminator so that they get better together.[^2]

[^2]: See [@wiki_gan].

### Generator {.unnumbered}

Our generator is a deep neural network with the following basic structure:

```{r, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Generator Structure"), echo=FALSE}
knitr::include_graphics("figure/gen_model.png")
```

The input of this model (figure 4) is a 128-length vector of Gaussian noise. The noise will go through four dense layers (currently set at 256 nodes each) with ReLU activation, dropout, and batch normalization. Then, the model splits in half, one each for pitch and duration generation. The pitch part of the model then upsamples the output from the last layer into $20\times128$ nodes, where 20 is for the 20 notes to generate, and 128 is for the one-hot vector of pitches. Similarly, we upsample duration from the same layer into $20\times2$ nodes, where 20 is for the 20 notes to generate and 2 is for start and end times. Afterwards, we reshaped these nodes into the correct shapes, (20, 128) for pitches and (20, 2) for duration, and apply the corresponding activation functions to the correct axis: softmax for pitch (for pitch onehot), and ReLU for duration (for outputting positive numbers). The generated pitch and duration vectors are then concatenated into one single array of dimensions (20, 130) as the output of the model.

Using this modeling structure, we ensure that pitch and duration of one generated music sample is generated by a single model and one single noise input. The dense layers preceding the split in the model will allow the model to learn latent features and rules of music before feeding that latent representation of the final product into the part of the model that turns latent vectors into pitch and duration. 

Conventionally, music generator models are usually time-dependent models such as the LSTM, with some added features like the attention mechanism for model to learn how to emphasize on repeated patterns. While those models work, they are more complex than simple dense neural networks deployed in our approach. We observe that, even with a more simplistic (and thus less artificial) model, the generator used in this research is still able to learn basic musical structures.

Once the generator made its predictions, we fed them through another function we wrote to convert the softmax vector for pitch into a one-hot vector, so that the output would be identical to the real samples in terms of format.

### Discriminator {.unnumbered}

The discriminator's job is to take a sample and judge whether this sample is real (composed by a human) or fake (generated by the generator). Our discriminator model is a simple dense neural network with input dimensions of $20\times130$ (for the 20-note sample, either generated or real) and outputs a float between 0 and 1 (using sigmoid activation) that represents the probability of whether the sample is fake (0) or real (1).

```{r, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Discriminator Structure"), echo=FALSE}
knitr::include_graphics("figure/dis_model.png")
```

As we can see, the model is a straightforward dense neural network (with dropout and batch normalization layers). The model is deliberately kept simple here because unequal speeds at which the generator and discriminator learn was a big concern during the training. More often than not in this case, the discriminator learns faster than the generator. Surpsingly, we found that a simple discriminator model provided an environment that was conducive to long-term learning between both the generator and discriminator.

### Adversarial Model {.unnumbered}

The adversarial model is simply the generator and discriminator organized in sequence as illustrated below:

```{r, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Adversarial Model Structure"), echo=FALSE}
knitr::include_graphics("figure/gan_model.png")
```

However, in the adversarial model, the weights of the discriminator are frozen because the error from the discriminator must be fed only to the generator in order to train the generator. We will expand on this topic in the next section.

### Training GAN models {.unnumbered}

Training GAN models is not a straightforward process and requires iterating through the following specialized steps:

1. **Train the discriminator with real samples.** We use a function to sample from the real training data. The size of this sample is usually 1/2 of the batch size parameter of the training script. We then feed these data and the target variable (a vector of 1s indicating they are real samples) into the discriminator model. The resulting error is the error of misclassifying real samples as fake. We use binary crossentropy as the loss metric.
2. **Train the discriminator with fake samples.** We use a function to generate fake samples using the generator model that we set up. The size of this sample is 1/2 of the batch size parameter of the training script. We then feed these data and the target variable (a vector of 0s indicating they are fake) to the discriminator model. The resulting error is the error of misclassifying fake samples as real. We use binary crossentropy as the loss metric.
3. **Train the adversarial model**. We feed the adversarial model with a number of batches of noise as input, but mark the target variable as a vector of 1's, or telling the model that these are real samples. The adversarial model is the generator and discriminator organized in sequence, so the random noise input will be converted into fake samples of music before getting fed into the discriminator, where the output is a classification of whether the sample is fake or real. Since we technically mislabeled the data by using 1s as the target variable, the error was negatively correlated with how well the discriminator performs — the better the discriminator did, the larger the error, thus the more the weights will be adjusted in the generator, and vice versa. The discriminator is not affected because its weights were frozen when training the adversarial model. This was expected to balance the rate of learning for the generator and the discriminator and should have prevented either of them from outpeforming the other. In many cases this held true; however, as training persisted for an extended period of time, it became increasingly difficult to maintain this balance.

GAN models are notoriously fickle and require special care to train properly.[^3] In our training script, we set up a mechanism to collect the errors from the discriminator and the generator in order to keep track of training progress and detect any frequently occurring learning issues, and most of our work focused on ways to address these issues. We will address our learning points in the Findings section.

[^3]: p.107, [@foster2019].


