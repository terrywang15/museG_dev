# Preliminary Content {.unnumbered}

## Acknowledgements {.unnumbered}

First of all, we would like to thank Yuri Balasanov, our Capstone advisor, for his kind support and guidance.

## Preface {.unnumbered}

*Introduction*

**Problem Statement** 
A 17 seconds long melody named "The Silver Scale" was the first ever music generated by computer, which was in the year 1957. Some of the very early algorithms used stochastic Markov Chain models for generation and rule filtering.
Since then, computer music has been gaining a lot of public attention.
The objective is to assist human musicians (composers, arrangers, producers etc) with the help of computer based music generation environments.

The concept is to let the Machine learn the music styles automatically with as little human intervention as possible. The motivation behind using Deep Learning models to generate music is ts generality. As opposed to a rule based or a grammar based system, Deep Learning can generate samples which inherit the music styles and introduce appropriate randomness. 

**Research Purpose** 
The purpose of this project is to develop a deep learning based music generator (instrumental music) which has a good understanding of the language of music and is able to generate human-like outputs, ultimately helping musicians and non-musicians alike to develop and refine musical ideas, and facilitate them in the music composition process. We divided the project into two parts, Generation and Validation. 

By doing this iteratively, the weights and parameters of the model will hopefully converge to a point at which the music generated is satisfactory to the user, who can use the output of the model, or save the weights for future use.  In a sense, we are democratizing deep learning to non-technical users wishing to generate new music ideas in a novel way. Moreover, we expect the algorithmic process to able to explore more music combinations than any individual, and thus provide a more holistic and creative approach to music creation.

**Variables and Scope** 

Python libraries like Music21 and PrettyMIDI are the toolkits we used to extract meaningful information from our MIDI files. 

Some initial EDA led us to reading the music files as music21.stream.Score objects, which is a stream subclass for handling multi-part music. We were able to extract all partStreams in a Score using parts.stream.  

The Stream object can then be used to identify:
1. Instruments
2. Key Signatures
3. Overlaps
4. Time Signatures
5. Measures

From the measure, we were then able to identify:
1. Notes
2. Chord

We did our initial EDA with metrics like the Pitch Histogram and other Composition Parameters Analysis. AnalyzedKey is a Key object with a few extra parameters and  correlationCoefficient shows how well this key fits the profile of a piece in that key. We analyzes Music time Signature, Expected Music Key, Music Key Confidence and other Music Key Alternatives based on correlation values.

We also extracted Harmonic Sequences and attempted Harmonic Reductions to be able to compare how similar or dissimilar different music files are from each other.
