# Preliminary Content {.unnumbered}

## Acknowledgements {.unnumbered}

First of all, we would like to thank Yuri Balasanov, our Capstone advisor, for his kind support and guidance.

## Preface {.unnumbered}

*Introduction*

**Problem Statement**

Perfecting the creative process is a goal across all forms of art (ex. music, photography, painting, singing, poetry, writing). Finding a place to start can be the most challenging (writer's block). A musician can go down a melodic path that leads to a dead end. The process of discovery and satisfaction by the creator is iterative trial and error. To use a model as an analogy, this would be similar to an infinite grid search for hyperparameters â€” something that is not practical in the field of machine learning. Astonishingly, musicians today have been able to create music with their bare intellect and network of inspiration.

While there are successful musicians in today's creative environment, we think there is always room for approvement. Our Deep Learning approach aims to be the catalyst for that improvement by handling the difficult and arduous task of discovery and ideation for musicians and individuals interested in creating music.  

**Research Purpose**

The purpose of the project is to develop a deep learning-based music generator (instrumental music) that has a good understanding of the language of music and can generate human-like outputs. Ultimately, the generator supports musicians and non-musicians alike in developing and refining musical ideas. We divided the model phase of the project into two parts: Generation and Validation.

By doing this iteratively, the weights and parameters of the model will hopefully converge to a point at which the music generated is satisfactory to the user, who can use the output of the model, or save the weights for future use. We expect the algorithmic process to able to explore more music combinations than any individual and thus provide a more holistic and creative approach to music creation.

**Variables and Scope** 

Python libraries like Music21 and PrettyMIDI are the toolkits we used to extract meaningful information from our midi files. 

Some initial exploratory data analysis (EDA) led us to reading the music files as music21.stream.Score objects, which is a stream subclass for handling multi-part music. We were able to extract all partStreams in a Score using parts.stream.  

The Stream object can then be used to identify:
1. Instruments
2. Key Signatures
3. Overlaps
4. Time Signatures
5. Measures

From the measure, we were then able to identify:
1. Notes
2. Chord

We did our initial EDA with metrics like the Pitch Histogram and other Composition Parameters Analysis. AnalyzedKey is a Key object with a few extra parameters and  correlationCoefficient shows how well this key fits the profile of a piece in that key. We analyzed music time Signature, expected music key, nusic key confidence and other music key alternatives based on correlation values.

We also extracted harmonic sequences and attempted harmonic reductions to be able to compare how similar or dissimilar different music files are from each other.
